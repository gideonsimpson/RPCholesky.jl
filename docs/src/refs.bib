
@misc{diaz_robust_2023,
	title = {Robust, randomized preconditioning for kernel ridge regression},
	url = {http://arxiv.org/abs/2304.12465},
	abstract = {This paper introduces two randomized preconditioning techniques for robustly solving kernel ridge regression (KRR) problems with a medium to large number of data points (\$10{\textasciicircum}4 {\textbackslash}leq N {\textbackslash}leq 10{\textasciicircum}7\$). The first method, RPCholesky preconditioning, is capable of accurately solving the full-data KRR problem in \$O(N{\textasciicircum}2)\$ arithmetic operations, assuming sufficiently rapid polynomial decay of the kernel matrix eigenvalues. The second method, KRILL preconditioning, offers an accurate solution to a restricted version of the KRR problem involving \$k {\textbackslash}ll N\$ selected data centers at a cost of \$O((N + k{\textasciicircum}2) k {\textbackslash}log k)\$ operations. The proposed methods solve a broad range of KRR problems and overcome the failure modes of previous KRR preconditioners, making them ideal for practical applications.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Díaz, Mateo and Epperly, Ethan N. and Frangella, Zachary and Tropp, Joel A. and Webber, Robert J.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12465 [cs, math, stat]},
	keywords = {Mathematics - Numerical Analysis, Statistics - Machine Learning, 68W20, 65F10, 65F55},
	annote = {Comment: 20 pages, 9 figures},
	file = {arXiv.org Snapshot:/Users/grs53/Zotero/storage/FY8HHF6Z/2304.html:text/html;Full Text PDF:/Users/grs53/Zotero/storage/T68HDPJ9/Díaz et al. - 2023 - Robust, randomized preconditioning for kernel ridg.pdf:application/pdf},
}

@misc{chen_randomly_2023,
	title = {Randomly pivoted {Cholesky}: {Practical} approximation of a kernel matrix with few entry evaluations},
	shorttitle = {Randomly pivoted {Cholesky}},
	url = {http://arxiv.org/abs/2207.06503},
	doi = {10.48550/arXiv.2207.06503},
	abstract = {Randomly pivoted Cholesky (RPCholesky) is a natural algorithm for computing a rank-k approximation of an N x N positive semidefinite (psd) matrix. RPCholesky can be implemented with just a few lines of code. It requires only (k+1)N entry evaluations and O(k{\textasciicircum}2 N) additional arithmetic operations. This paper offers the first serious investigation of its experimental and theoretical behavior. Empirically, RPCholesky matches or improves on the performance of alternative algorithms for low-rank psd approximation. Furthermore, RPCholesky provably achieves near-optimal approximation guarantees. The simplicity, effectiveness, and robustness of this algorithm strongly support its use in scientific computing and machine learning applications.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Chen, Yifan and Epperly, Ethan N. and Tropp, Joel A. and Webber, Robert J.},
	month = feb,
	year = {2023},
	note = {arXiv:2207.06503 [cs, math, stat]},
	keywords = {Mathematics - Numerical Analysis, Statistics - Machine Learning, 65F55, 65C99, 68T05},
	annote = {Comment: 34 pages, 4 figures},
	file = {arXiv Fulltext PDF:/Users/grs53/Zotero/storage/ZDD676W5/Chen et al. - 2023 - Randomly pivoted Cholesky Practical approximation.pdf:application/pdf;arXiv.org Snapshot:/Users/grs53/Zotero/storage/NVC5DNZD/2207.html:text/html},
}
