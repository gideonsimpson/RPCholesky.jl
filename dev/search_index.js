var documenterSearchIndex = {"docs":
[{"location":"krr1/#Kernel-Ridge-Regression","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"","category":"section"},{"location":"krr1/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"One of the key applications of the method is to solving the ridge regressino problem.  This is performed with","category":"page"},{"location":"krr1/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"    ridge_rpcholesky","category":"page"},{"location":"krr1/#RPCholesky.ridge_rpcholesky","page":"Kernel Ridge Regression","title":"RPCholesky.ridge_rpcholesky","text":"ridge_rpcholesky(A, y, k, λ; τ=1e-8, verbose=false)\n\nUse RP Cholesky to accelerate ridge regression as in Algorithm 2.3 of Chen et al.\n\nThis returns the approximate solution of (A + N λ I) β = y along with the pivots from the factorization. \n\nFields\n\nA - the matrix, assumed to be spd\ny - the right handside vector\nk - maximum rank of approximation \nλ - ridge regression regularizaiton parameter\nτ = 1e-8 - approximation tolerance\nverbose = false - provide diagnostic details\n\n\n\n\n\n","category":"function"},{"location":"#RPCholesky.jl-Documentation","page":"Home","title":"RPCholesky.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Randomly Pivoted Cholesky algorithm, implemented in Julia, based on the description in https://arxiv.org/abs/2207.06503 by Chen et al. ","category":"page"},{"location":"#Acknowledgements","page":"Home","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package has been developed in conjunction with R.J. Webber and D. Aristoff.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This work was supported in part by the US National Science Foundation Grant DMS-2111278.","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Y. Chen, E. N. Epperly, J. A. Tropp, and R. J. Webber, “Randomly pivoted Cholesky: Practical approximation of a kernel matrix with few entry evaluations.” arXiv, Feb. 22, 2023. doi: 10.48550/arXiv.2207.06503.\nM. Díaz, E. N. Epperly, Z. Frangella, J. A. Tropp, and R. J. Webber, “Robust, randomized preconditioning for kernel ridge regression.” arXiv, Aug. 02, 2023. doi: 10.48550/arXiv.2304.12465.","category":"page"},{"location":"rpc1/#RPCholesky-Factorization","page":"Factorization","title":"RPCholesky Factorization","text":"","category":"section"},{"location":"rpc1/","page":"Factorization","title":"Factorization","text":"Given a spd matrix A, the rank k random Cholesky factorization can be obtained with ","category":"page"},{"location":"rpc1/","page":"Factorization","title":"Factorization","text":"    rpcholesky","category":"page"},{"location":"rpc1/#RPCholesky.rpcholesky","page":"Factorization","title":"RPCholesky.rpcholesky","text":"rpcholesky(A, k; τ = 1e-8, verbose = false)\n\nPerform the RPCholesky factorization for matrix A of rank at most k.  This corresponds to Algorithm 2.1 of Chen et al.\n\nThis returns the factorization F, along with the array of pivots, S.\n\nFields\n\nA - an spd matrix\nk - maximum desired rank\nτ = 1e-8 - approximation tolerance\nverbose = false - provide diagnostic details\n\n\n\n\n\n","category":"function"},{"location":"rpc1/","page":"Factorization","title":"Factorization","text":"Its usage is illustrated in the following simple example, which returns teh factorization Aapprox FF","category":"page"}]
}
